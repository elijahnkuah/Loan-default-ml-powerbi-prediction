# -*- coding: utf-8 -*-
"""loan_default_df_21 - Copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sqdBJh_rJZ3oZzsAnit92gE7bE3GkW9Y

# Hacker Earth Machine Learning Challenge 

## Problem Statement
The Bank Indessa has not done well in last 3 quarters. Their NPAs (Non Performing Assets) have reached all time high. It is starting to lose confidence of its investors. As a result, itâ€™s stock has fallen by 20% in the previous quarter alone.
After careful analysis, it was found that the majority of NPA was contributed by loan defaulters. With the messy data collected over all the years, this bank has decided to use machine learning to figure out a way to find these defaulters and devise a plan to reduce them.
This bank uses a pool of investors to sanction their loans. For example: If any customer has applied for a loan of $20000, along with bank, the investors perform a due diligence on the requested loan application. Keep this in mind while understanding data.
In this challenge, you will help this bank by predicting the probability that a member will default.


### Solution Approach
"""

from google.colab import drive
drive.mount('/content/drive')

pip install optbinning

"""# Importing needful libraries"""

import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score, log_loss
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import spearmanr
from scipy.stats import chi2_contingency
from sklearn.model_selection import StratifiedKFold
import xgboost as xgb
from sklearn.impute import SimpleImputer as Imputer
from sklearn import preprocessing
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
import re
import timeit
import random
from optbinning import BinningProcess
from optbinning import Scorecard
from optbinning.scorecard import plot_auc_roc, plot_cap, plot_ks
random.seed(3)

dataset = pd.read_csv("/content/drive/MyDrive/scorecard/datasets/dataset.csv")

pd.options.display.max_columns = None
pd.options.display.max_rows = None

#dataset = dataset.sample(frac = 0.1, random_state=10)

#pd.DataFrame(dataset).to_csv('datasets/dataset.csv', index=False)

dataset.head()

dataset.info()

dataset.isnull().sum()

dataset.columns

y = dataset.loan_status.values
y.shape

"""## VISUALISATION"""

plt.figure(figsize = [12,8])
sns.countplot(x = 'loan_status', data = dataset)

plt.figure(figsize=(22,8))
sns.countplot(x = 'purpose', data = dataset)

#dataset['home_ownership'].values_count()

plt.figure(figsize=(20,8))
sns.countplot(x = 'home_ownership', data = dataset)

plt.figure(figsize=(20,10))
#cor = dataset.
cor = dataset.drop('member_id', axis=1).corr(method = 'spearman')
fig = sns.heatmap(cor, annot=True)
plt.show(fig)

#correlation, pval = spearmanr(dataset.drop("member_id",axis=1))
#print(f'correlation={correlation:.6f},p-value={pval:.6f}')
#from scipy.stats import rankdata
#sns.jointplot(x=dataset['emp_length'], y=dataset['loan_amnt'])

print(dataset.drop("member_id", axis=1).corr(method='spearman'))

"""## DATA CLEANING

## Droping some unwanted dependent variables
"""

### columns undropped from original data ['grade','addr_state', 'pymnt_plan','verification_status','delinq_2yrs',]

drop_col = ['member_id','funded_amnt', 'funded_amnt_inv', 'batch_enrolled','sub_grade', 'mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog','emp_title', 'desc','title','zip_code','verification_status_joint','last_week_pay','loan_status','recoveries']

dataset_1 = dataset.drop(['member_id','funded_amnt', 'funded_amnt_inv', 'batch_enrolled','mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog','sub_grade', 'emp_title', 'desc','title','zip_code','verification_status_joint','last_week_pay','loan_status','recoveries'], axis=1)

dataset_1.shape

dataset_1.emp_length.unique()

columns  = dataset_1.columns
columns

dataset_1.application_type.unique()

"""### Converting objects variables to integers"""

dataset_1['emp_length'] = dataset_1['emp_length'].replace({np.nan:0, '< 1 year':1, '1 year':2, '2 years':3, '3 years':4, '4 years':5, '5 years':6, '6 years':7, '7 years':8, '8 years':9, '9 years':10, '10+ years':11})

dataset_1['term'] = dataset_1['term'].replace({'36 months':36, '60 months':60})

dataset_1['home_ownership']=dataset_1['home_ownership'].replace({'OWN':6, 'MORTGAGE':5, 'RENT':4, 'OTHER':3, 'NONE':2, 'ANY':1})

dataset_1['purpose'] = dataset_1['purpose'].replace({'small_business':1,'debt_consolidation':2, 'home_improvement':3, 'credit_card':4,'major_purchase':5,  
                                                     'vacation':6, 'car':7, 'moving':8,'medical':9, 'wedding':10, 'renewable_energy':11, 'house':12, 'educational':13,'other':14,})
dataset_1['application_type'] = dataset_1['application_type'].replace({'INDIVIDUAL':1, 'JOINT':2})

dataset_1['verification_status'] = dataset_1['verification_status'].replace({'Source Verified':1, 'Not Verified':0, 'Verified':1})

dataset_1['grade'] = dataset_1['grade'].replace({'E':2, 'B':5, 'A':6, 'D':3, 'C':4, 'F':1, 'G':0})

dataset_1['pymnt_plan'] = dataset_1['pymnt_plan'].replace({'n':0, 'y':1})

dataset_1['initial_list_status'] = dataset_1['initial_list_status'].replace({'f':1, 'w':0})

dataset_1['addr_state'] = dataset_1['addr_state'].replace({'ND':0, 'NE':0,'IA':0,'NV':0,'FL':0,'HI':0,'AL':0,'NY':0,'NM':1,'VA':1,'OK':2,'TN':2,'MO':2,'LA':2,'MD':2,'NC':2,'UT':3,
                 'KY':3,'AZ':3,'NJ':3,'AR':4,'MI':4,'PA':4,'OH':4,'MN':4,'CA':4,'RI':5,'MA':5,'DE':5,'SD':5,'IN':5,'GA':6,'WA':6,'OR':6,
                 'WI':7,'MT':7,'IL':8,'CT':8,'KS':9,'SC':9,'CO':9,'VT':9,'AK':9,'MS':9,'TX':9,'WV':10,'NH':10,'WY':10,'DC':10,'ME':10,'ID':10})

dataset_1.addr_state.unique()

"""## New features"""

dataset_1['term'] = dataset_1['term'].apply(lambda x: float(x))

value = -9999
def helping_features(value):
    i = ((dataset_1['int_rate'])/100)/12
    d = 1 - (1/(1+i)**dataset_1['term'])
    dataset_1['Monthly_supposed_payment'] = (dataset_1['loan_amnt']*i)/d
    dataset_1['Total_refund'] = dataset_1['Monthly_supposed_payment']*dataset_1['term']
    #dataset['Interest_amnt'] = dataset['Total_refund'] - dataset['loan_amnt']
    dataset_1['Monthly_income'] = dataset_1['annual_inc'].apply(lambda x : x/12 if x >=0 else -9999)

helping_features(-9999)
dataset_1.head()

dataset_1.collections_12_mths_ex_med.unique()

"""## Filling missing values"""

dataset_1.isnull().sum()

def fill_nulls(value):
    cols_fill = ['collections_12_mths_ex_med','revol_util', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']
    
    if value == -9999:
        for col in cols_fill:
            dataset_1.loc[dataset_1[col].isnull(), col] = -9999
    else : 
        for col in cols_fill:
            dataset_1.loc[dataset_1[col].isnull(), col] = dataset_1[col].mean()

fill_nulls(-9999)

columns = dataset_1.columns
columns
variable_names = ['loan_amnt', 'term', 'int_rate', 'grade', 'emp_length',
       'home_ownership', 'annual_inc', 'verification_status', 'pymnt_plan',
       'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'inq_last_6mths',
       'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',
       'initial_list_status', 'total_rec_int', 'total_rec_late_fee',
       'collection_recovery_fee', 'collections_12_mths_ex_med',
       'application_type', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',
       'total_rev_hi_lim', 'Monthly_supposed_payment', 'Total_refund',
       'Monthly_income']

dataset_1.info()

"""## SCALING THE DATASET """

columns = dataset_1.columns
columns

# Use 3 features
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer
from sklearn.feature_selection import SelectKBest, f_classif
df = SelectKBest(f_classif, k=3)
df1 = StandardScaler()
data_scale = df1.fit_transform(dataset_1) 
data = pd.DataFrame(data_scale, columns=columns)
data.head()

"""### Spliting the test and trian data"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(data,y, test_size=0.2, random_state=0)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

"""# Training the Data - Model"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(max_iter=1000)
classifier = classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

y_pred_prob = classifier.predict_proba(X_test)[:,1]
y_pred_prob

from sklearn import metrics
accuracy = metrics.accuracy_score(y_test, y_pred)
accuracy

classifier.coef_

"""### Manipulating the parameters - second model"""

model = LogisticRegression(max_iter=1000)
solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2']
c_values = [100, 10, 1.0, 0.1, 0.01]
# define grid search
grid = dict(solver=solvers,penalty=penalty,C=c_values)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(X_train, y_train)

y_pred_1 = grid_result.predict(X_test)

accuracy_2 = metrics.accuracy_score(y_test, y_pred_1)
accuracy_2

classifier.coef_

metrics.confusion_matrix(y_test, y_pred_1)

"""### XGBOOST MODEL"""

# xgboost for regression
from xgboost import XGBClassifier
model_xgb = XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='mlogloss')
model_xgb.fit(X_train, y_train)
y_pred_xgb = model_xgb.predict(X_test)

y_pred_prob_ = model_xgb.predict_proba(X_test)
y_pred_prob_

y_pred_prob_1 = pd.DataFrame(y_pred_prob_)

y_pred_prob_1.head()

y_pred_prob = y_pred_prob_1.iloc[:,1]

y_pred_prob.head()

accuracy_xgb = metrics.accuracy_score(y_test, y_pred_xgb)
print(accuracy_xgb)
"{:.2f}".format(accuracy_xgb)

"""# Save Our Model
* Serialization
* Pickle
* Joblib
* numpy/json/ray
"""

# Using Joblib
import joblib

model_file = open("/content/drive/MyDrive/scorecard/models/classifier_model.pkl","wb")
joblib.dump(classifier,model_file)
model_file.close()

model_file = open("/content/drive/MyDrive/scorecard/models/classifier_model_2.pkl","wb")
joblib.dump(grid_result,model_file)
model_file.close()

"""Prediction = pd.DataFrame({
    'member_id' : member_id,
    'loan_status' : stack_result_prob
})
Prediction = Prediction[['member_id','loan_status']]

Prediction.to_csv('datasets/Prdeiction.csv', index = False)

# CREDIT SCORE NEW
"""

#special_codes = [-9, -8, -7]

selection_criteria = {
    "iv": {"min": 0.02, "max": 1},
    "quality_score": {"min": 0.01}
}

binning_process = BinningProcess(variable_names)

estimator = LogisticRegression(solver="lbfgs")

scorecard = Scorecard(binning_process=binning_process,
                      estimator=estimator, scaling_method="min_max",
                      scaling_method_params={"min": 300, "max": 850})

scorecard.fit(X_train, y_train)

scorecard.information(print_level=2)

pd.options.display.max_rows = 20

scorecard.table(style="summary")

scorecard.table(style="detailed")

sc = scorecard.table(style="summary")

sc.groupby("Variable").agg({'Points' : [np.min, np.max]}).sum()

y_pred = scorecard.predict_proba(X_test)[:, 1]

plot_auc_roc(y_test, y_pred)

plot_cap(y_test, y_pred)

score = scorecard.score(X_test)
pd.DataFrame(score).head()

plot_ks(y_test, y_pred)

mask = y == 0
plt.hist(score[~mask], label="non-event", color="b", alpha=0.35)
plt.hist(score[~mask], label="event", color="r", alpha=0.35)
plt.xlabel("score")
plt.legend()
plt.show()



"""# Creating the summary table"""

feature_name = ['loan_amnt', 'term', 'int_rate', 'grade', 'emp_length',
       'home_ownership', 'annual_inc', 'verification_status', 'pymnt_plan',
       'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'inq_last_6mths',
       'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',
       'initial_list_status', 'total_rec_int', 'total_rec_late_fee',
       'recoveries', 'collection_recovery_fee', 'collections_12_mths_ex_med',
       'application_type', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',
       'total_rev_hi_lim', 'Monthly_supposed_payment', 'Total_refund',
       'Monthly_income']

summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)
summary_table['Coefficients'] = np.transpose(classifier.coef_)
summary_table.index = summary_table.index + 1
summary_table.loc[0] = ['Intercept', classifier.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table

inputs_train = X_train
X_train.head()

inputs_test = X_test
X_test.head()

#y_hat_test = classifier.predict(inputs_test)
#y_hat_test

y_pred_prob

y_hat_test_proba = y_pred_prob
y_hat_test_proba

y_test = pd.DataFrame(y_test)
y_test.tail()

loan_data_targets_test = y_test
loan_data_targets_test.head()

df_actual_predicted_probs = pd.concat([loan_data_targets_test, pd.DataFrame(y_hat_test_proba)], axis = 1)
# Concatenates two dataframes.

df_actual_predicted_probs.columns = ['loan_data_targets_test', 'y_hat_test_proba']

df_actual_predicted_probs.head()

"""## Accuracy and Area under the Curve"""

tr = 0.5
# We create a new column with an indicator,
# where every observation that has predicted probability greater than the threshold has a value of 1,
# and every observation that has predicted probability lower than the threshold has a value of 0.
df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)

pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])
# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.
# This table is known as a Confusion Matrix.

pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]
# Here we divide each value of the table by the total number of observations,
# thus getting percentages, or, rates.

(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]
# Here we calculate Accuracy of the model, which is the sum of the diagonal rates.

from sklearn.metrics import roc_curve, roc_auc_score

roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])
# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.
# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds.

fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])
# Here we store each of the three arrays in a separate variable.

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

plt.plot(fpr, tpr)
# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,
# thus plotting the ROC curve.
plt.plot(fpr, fpr, linestyle = '--', color = 'k')
# We plot a seconary diagonal line, with dashed line style and black color.
plt.xlabel('False positive rate')
# We name the x-axis "False positive rate".
plt.ylabel('True positive rate')
# We name the x-axis "True positive rate".
plt.title('ROC curve')
# We name the graph "ROC curve".

"""# Accuracy"""

AUROC = roc_auc_score(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])
# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)
# from a set of actual values and their predicted probabilities.
'{:.2f}'.format(AUROC)

"""## Gini and Kolmogorov-Smirnov"""

df_actual_predicted_probs = df_actual_predicted_probs.sort_values('y_hat_test_proba')
# Sorts a dataframe by the values of a specific column.

df_actual_predicted_probs.head()

df_actual_predicted_probs.tail()

df_actual_predicted_probs = df_actual_predicted_probs.reset_index()
# We reset the index of a dataframe and overwrite it.

df_actual_predicted_probs.head()

df_actual_predicted_probs['Cumulative N Population'] = df_actual_predicted_probs.index + 1
# We calculate the cumulative number of all observations.
# We use the new index for that. Since indexing in ython starts from 0, we add 1 to each index.
df_actual_predicted_probs['Cumulative N Good'] = df_actual_predicted_probs['loan_data_targets_test'].cumsum()
# We calculate cumulative number of 'good', which is the cumulative sum of the column with actual observations.
df_actual_predicted_probs['Cumulative N Bad'] = df_actual_predicted_probs['Cumulative N Population'] - df_actual_predicted_probs['loan_data_targets_test'].cumsum()
# We calculate cumulative number of 'bad', which is
# the difference between the cumulative number of all observations and cumulative number of 'good' for each row.

df_actual_predicted_probs['Cumulative Perc Population'] = df_actual_predicted_probs['Cumulative N Population'] / (df_actual_predicted_probs.shape[0])
# We calculate the cumulative percentage of all observations.
df_actual_predicted_probs['Cumulative Perc Good'] = df_actual_predicted_probs['Cumulative N Good'] / df_actual_predicted_probs['loan_data_targets_test'].sum()
# We calculate cumulative percentage of 'good'.
df_actual_predicted_probs['Cumulative Perc Bad'] = df_actual_predicted_probs['Cumulative N Bad'] / (df_actual_predicted_probs.shape[0] - df_actual_predicted_probs['loan_data_targets_test'].sum())
# We calculate the cumulative percentage of 'bad'.

df_actual_predicted_probs.head()

# Plot Gini
plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Bad'])
# We plot the cumulative percentage of all along the x-axis and the cumulative percentage 'good' along the y-axis,
# thus plotting the Gini curve.
plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Population'], linestyle = '--', color = 'k')
# We plot a seconary diagonal line, with dashed line style and black color.
plt.xlabel('Cumulative % Population')
# We name the x-axis "Cumulative % Population".
plt.ylabel('Cumulative % Bad')
# We name the y-axis "Cumulative % Bad".
plt.title('Gini')
# We name the graph "Gini".

Gini = AUROC * 2 - 1
# Here we calculate Gini from AUROC.
Gini

# Plot KS
plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Bad'], color = 'r')
# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'bad' along the y-axis,
# colored in red.
plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Good'], color = 'b')
# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'good' along the y-axis,
# colored in red.
plt.xlabel('Estimated Probability for being Good')
# We name the x-axis "Estimated Probability for being Good".
plt.ylabel('Cumulative %')
# We name the y-axis "Cumulative %".
plt.title('Kolmogorov-Smirnov')
# We name the graph "Kolmogorov-Smirnov".

KS = max(df_actual_predicted_probs['Cumulative Perc Bad'] - df_actual_predicted_probs['Cumulative Perc Good'])
# We calculate KS from the data. It is the maximum of the difference between the cumulative percentage of 'bad'
# and the cumulative percentage of 'good'.
KS

"""# Creating a Scorecard"""

summary_table

min_score = 300
max_score = 850

df_scorecard = summary_table

df_scorecard.groupby('Feature name')['Coefficients'].min()
# Groups the data by the values of the 'feature name' column.
# Aggregates the data in the 'Coefficients' column, calculating their minimum.

min_sum_coef_1 = df_scorecard['Coefficients'].min()
# Up to the 'min()' method everything is the same as in te line above.
# Then, we aggregate further and sum all the minimum values.
min_sum_coef = -1.5386183729406917

max_sum_coef_1 = df_scorecard['Coefficients'].max()
# Up to the 'min()' method everything is the same as in te line above.
# Then, we aggregate further and sum all the minimum values.
max_sum_coef = 5.590270189646491

df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)
# We multiply the value of the 'Coefficients' column by the ration of the differences between
# maximum score and minimum score and maximum sum of coefficients and minimum sum of cefficients.
df_scorecard

df_scorecard['Score - Calculation'][0] = ((df_scorecard['Coefficients'][0] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score
# We divide the difference of the value of the 'Coefficients' column and the minimum sum of coefficients by
# the difference of the maximum sum of coefficients and the minimum sum of coefficients.
# Then, we multiply that by the difference between the maximum score and the minimum score.
# Then, we add minimum score. 
df_scorecard

df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()
# We round the values of the 'Score - Calculation' column.
df_scorecard

df_scorecard['Score - Preliminary'] = abs(df_scorecard['Score - Preliminary'])
df_scorecard



X_test_intercept = X_test.insert(0, 'Intercept', 1)
# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.
# The name of that column is 'Intercept', and its values are 1s.
X_test.head()

X_test_intercept.shape

X_test.shape

scorecard_scores = df_scorecard['Score - Final']

scorecard_scores.shape

scorecard_scores = scorecard_scores.values.reshape(34, 1)
scorecard_scores.shape

y_scores = X_test_intercept.dot(scorecard_scores)
# Here we multiply the values of each row of the dataframe by the values of each column of the variable,
# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products.
y_scores.head()

"""### From Credit Score to PD"""

sum_coef_from_score = ((y_scores - min_score) / (max_score - min_score))
# We divide the difference between the scores and the minimum score by
# the difference between the maximum score and the minimum score.
# Then, we multiply that by the difference between the maximum sum of coefficients and the minimum sum of coefficients.
# Then, we add the minimum sum of coefficients.
sum_coef_from_score

y_hat_proba_from_score = np.exp(sum_coef_from_score) / (np.exp(sum_coef_from_score) + 1)
# Here we divide an exponent raised to sum of coefficients from score by
# an exponent raised to sum of coefficients from score plus one.
y_hat_proba_from_score.head()

y_hat_test_proba[0: 5]

















